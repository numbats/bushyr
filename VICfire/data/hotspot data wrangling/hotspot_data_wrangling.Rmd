---
title: "hotspot-data-wrangling"
author: "Helen Evangelina"
date: "25/11/2021"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(leaflet)
library(readr)
library(KernSmooth)
library(shinyWidgets)
library(plotly)
library(sp)
library(mapview)
library(leafem)
library(rgdal)
library(maptools)
library(raster)
library(DT)
library(htmlwidgets)
library(lubridate)
library(forcats)
library(purrr)
library(sf)
```

# data processing
-- This step is to wrangle the data taken from the FTP 

```{r, eval = FALSE}
#getting the directories with daily folder inside
pre <- list.files("data/hotspot", "monthly", recursive=TRUE, full.names=TRUE, include.dirs=TRUE)
pre

#getting the file directories inside the daily folder
fls <- list.files(pre, full.names = TRUE)
fls

all <- map(fls, read_csv)
combined_data <- bind_rows(all)

combined_data
```

```{r, eval = FALSE}
# filtering the data
filtered_data <- combined_data %>%
    filter(between(lon, 112, 155)) %>%
    filter(between(lat, -44, -10))

# filtering hotspots by firepowewr
filtered_data <- filtered_data %>% 
  filter(firepower > 100)

# transforming hotspots data to sf object
filtered_data2 <- st_as_sf(x = filtered_data, coords = c('lon', 'lat'), crs = 4326)
```

```{r, eval = FALSE}
# filtering by target states
library(rnaturalearth)
au_map <- ne_states(country = 'Australia', returnclass = 'sf')

states <- st_intersects(au_map$geometry, filtered_data2$geometry)

filtered_data2$state <-  ''

  for (i in seq(1,nrow(au_map))){

  filtered_data2$state[states[[i]]] <-  au_map$name[i]

  }
```

```{r, eval = FALSE}
target_area <-  c('Victoria',
                  'New South Wales',
                  'Northern Territory',
                  'South Australia',
                  'Western Australia',
                  'Queensland',
                  'Tasmania',
                  'Australian Capital Territory')

target_area <- target_area[1:1]

filtered_data2 <- filtered_data2 %>%
  filter(state %in% target_area)

```

```{r, eval = FALSE}
# Convert hotspots data back to data frame
coords <- st_coordinates(filtered_data2)
st_geometry(filtered_data2) <- NULL
filtered_data2$lon = coords[,1]
filtered_data2$lat = coords[,2]
```


```{r, eval = FALSE}
# trimming the date 
hotspots <- filtered_data2 %>%
  mutate(date = substr(filtered_data2$`#obstime`, 2, 20))
```


```{r, eval = FALSE}
# extract time features
hotspots$year <-  year(as.POSIXct(hotspots$date))
hotspots$month <-  month(hotspots$date)
  hotspots$day <-  day(hotspots$date)
  hotspots$week <-  week(hotspots$date)
  hotspots$hour <-  hour(hotspots$date)

```

```{r, eval = FALSE}
hotspots <- arrange(hotspots, year, month, day, hour)

hotspots$hour_id <- difftime(hotspots$date, min(hotspots$date), units = "hour") %>%
    as.numeric() %>%
    round() %>%
    as.integer()

hotspots$hour_id <- hotspots$hour_id + 1

hotspots_trim_all <- hotspots %>%
  mutate(time_id = hour_id) %>%
  mutate(id = 1:nrow(hotspots)) %>%
  select(id, lon, lat, time_id)

hotspots_trim_all
```

```{r, eval = FALSE}
# filtering to only 2018
hotspots_2018 <- hotspots %>%
  filter(year == 2018)
```


```{r, eval = FALSE}
# splitting the data to month 1-3 and month 10-12
hotspots_13 <- hotspots %>%
  filter(month %in% c(1,2,3))

hotspots_trim_13 <- hotspots_13 %>%
  mutate(time_id = hour_id) %>%
  mutate(id = 1:nrow(hotspots_13)) %>%
  select(id, lon, lat, time_id)

hotspots_1012 <- hotspots %>%
  filter(month %in% c(10, 11, 12))

hotspots_trim_1012 <- hotspots_1012 %>%
  mutate(time_id = hour_id) %>%
  mutate(id = 1:nrow(hotspots_1012)) %>%
  select(id, lon, lat, time_id)
```

```{r, eval = FALSE}
# writing the files
write_csv(hotspots_trim_all, here::here("data/VIC_hotspots_before_clustering.csv"))
write_csv(hotspots, here::here("data/VIC_hotspots_raw.csv"))
```



## Clustering
This section provides the clustering processes. Final data resulted from this is: 
- predict_x_2016_2018.csv: for the bushifre period 2016-2017, 2017-2018, 2018-2019
 - predict_x_2019_2020.csv: for the bushfire period 2019-2020 and 2020-2021
These 2 data can be found inside the "clustering" folder inside data.
```{r}
library(reticulate)
```

### clustering parameters-tuning (to get clustering_grid.csv)
```{r, eval = FALSE}
system("python scripts/Clustering/clustering_tune.py")
```

```{python, eval = FALSE}
from numpy import genfromtxt

def read_hotspots_from_csv(file_path = 'data/VIC_hotspots_before_clustering.csv'):
	"""
	Read hotspots from the csv file.
	Input: 
		file_path: file_path and name of the csv file
	Output:
		ID: the unique indentifier of the hotspot
		lon: longitude
		lat: latitude
		time_id: observered time formatted in indexes (discrete & integer values)
	"""
	data = genfromtxt(
		file_path, 
		delimiter = ',', 
		skip_header = 1, 
		dtype = [('id', 'int32'), ('lon', 'float32'), ('lat', 'float32'), ('time_id', 'int32')]
		)

	ID = data['id']
	time_id = data['time_id']
	lon = data['lon']
	lat = data['lat']

	return ID, lon, lat, time_id
	
read_hotspots_from_csv()
```

```{python, eval = FALSE}
from tqdm import tqdm
import numpy as np

class CLUSTERER:
	
	def __init__(self, active_time = 24, adj_dist = 3000):
		"""
		Initialize the instance
		Input:
			active_time: length of time bushfires remain active
			adj_dist: density-based distance used in the clustering algorithm
		"""

		self.active_time = active_time
		self.adj_dist = adj_dist
		self.memberships = None

	def cluster(self, ID, lon, lat, time_id):
		"""
		Cluster hotspots into bushfires
		Input:
			self: instance
			ID: unique indetifier
			lon: a vector of longitude
			lat: a vector of latitude
			time_id: observered time formatted in indexes (discrete & integer values)
		"""

		# Compute the maximum timestamps
		self.maxtime = np.max(time_id).tolist()

		# Initialize the memberships
		self.memberships = np.zeros(len(lon), dtype = "int32")


		# Loop through every timestamp
		for i in tqdm(range(1, self.maxtime + 1), ascii = True):
			if np.any(time_id == i):

				# Extract all revelant timestamps and compute the memberships
				indexes = np.where((time_id <= i) & (max(1, i - self.active_time) <= time_id))
				temp_memberships = np.zeros(len(lon), dtype = "int32")
				temp_memberships[indexes] = self.compute_memberships(lon[indexes], lat[indexes])

				# Define current timestamp and past timestamps
				current = np.where(time_id == i)
				past = np.where((time_id < i) & (max(1, i - self.active_time) <= time_id))

				# No previous points
				if len(current[0]) == len(indexes[0]):
					self.memberships[indexes] = temp_memberships[indexes] + np.max(self.memberships)

				else:

					# Assign memberships to points in current timestamp
					self.adjust_memberships(
						temp_memberships,
						current,
						past,
						lon,
						lat
						)

	def adjust_memberships(self, temp_memberships, current, past, lon, lat):
		"""
		Adjust memberships in current timestamp based on previous clustering result
		Input:
			temp_memberships: memberships with incorrect labels
			current: current timestamp
			past: past timestamps
			lon: a vector of longitude
			lat: a vector of latitude
		"""

		# Convert lon and lat to radians
		rc_lon = np.radians(lon[current])
		rc_lat = np.radians(lat[current])
		rp_lon = np.radians(lon[past])
		rp_lat = np.radians(lat[past])

		# Define the correctly labelled membership in current and past timestamps
		correct_c_memberships = np.zeros(len(rc_lon), dtype = "int32")
		correct_p_memberships = self.memberships[past]

		# Collect current incorrect memberships
		c_memberships = temp_memberships[current]
		p_memberships = temp_memberships[past]

		# New clusters
		new_clusters = []

		# Loop through every points in current timestamp
		for i in range(len(rc_lon)):

			# If the point in current timestamp share the same cluster with a point in previous timestamps
			if np.any(c_memberships[i] == p_memberships):

				# Compute the distance vector
				distance = self.point_to_vector_geodist(rc_lon[i], rc_lat[i], rp_lon, rp_lat)

				if len(p_memberships == c_memberships[i]) != len(distance):
					raise Exception("The length of distance does not match with the length of memberships")

				# Assign missing value to points not sharing the same cluster
				distance[p_memberships != c_memberships[i]] = np.nan

				# Select the non nan minium element, assign its correct past membership to the point in current timestamp 
				correct_c_memberships[i] = correct_p_memberships[distance == np.nanmin(distance)][0]

			else:

				new_clusters.append(i)

		# Get new clusters incorrect labels
		correct_c_memberships[new_clusters] = self.revise_memberships(c_memberships[new_clusters])
		correct_c_memberships[new_clusters] = correct_c_memberships[new_clusters] + np.max(self.memberships)

		if np.any(correct_c_memberships == 0):
			raise Exception("correct_c_memberships contains zeros")
		# Assign them back to the global memberships vector
		self.memberships[current] = correct_c_memberships


	def revise_memberships(self, memberships):
		"""
		Revise memberships to let it start from 1
		Input:
			memberships: a vector of memberships
		"""

		# Build a replacement dict
		dic = {key: index + 1 for index, key in enumerate(np.unique(memberships).tolist())}

		# Replace the memberships
		memberships = memberships.tolist()
		for i in range(len(memberships)):
			memberships[i] = dic[memberships[i]]

		return np.array(memberships, dtype = "int32")






	def compute_memberships(self, lon, lat):
		"""
		Compute memberships using Breadth First Search
		Input:
			lon: a vector of longitude
			lat: a vector of latitude
		Output:
			memberships: a vector of memberships
		"""

		# Define a membership vector
		memberships = np.zeros(len(lon), dtype = "int32")

		# If there is only one point, assign 1, then return
		if len(memberships) == 1:
			memberships[0] = 1
			return memberships

		# Convert lon and lat to radians
		rlon = np.radians(lon)
		rlat = np.radians(lat)

		# Define label number
		label = 0

		# While there are still unlabelled points
		while np.any(memberships == 0):

			# print(np.count_nonzero(memberships == 0))

			# Define a new label
			label = label + 1

			# Find the first unlabelled point
			first_point = np.where(memberships == 0)[0][0]

			# Push the first point to the queue
			queue = np.array(first_point, dtype = "int32")
			queue = np.reshape(queue, (1))

			# Label the first point
			memberships[queue[0]] = label


			# Set head and tail pointer
			head = 0
			tail = 0

			# Start BFS
			while head <= tail:

				# print("    queue:", len(queue))
				if len(queue) > len(lon):
					raise Exception("length of queue out of boundary")
				# print(queue)
				# print(memberships)

				# Computer the distance vector
				distance = self.point_to_vector_geodist(
					rlon[queue[head]],
					rlat[queue[head]],
					rlon,
					rlat
					)

				if len(distance) != len(memberships):
					raise Exception("The length of distance does not match with the length of memberships")

				# Get the extensions of the point
				extensions = np.where((distance <= self.adj_dist) & (memberships == 0))[0]

				# print(distance)
				# print(extensions)

				# Append extensions to queue
				queue = np.append(queue, extensions)

				# Label extensions points
				memberships[extensions] = label

				# Update tail
				tail = tail + len(extensions) 

				# Update head
				head = head + 1

		return memberships




	def point_to_vector_geodist(self, P_rlon, P_rlat, V_rlon, V_rlat):
		"""
		Using Haversine formulat to compute the distance from a point to a vector of points.
		Adpoted from https://stackoverflow.com/questions/19412462/getting-distance-between-two-points-based-on-latitude-longitude.
		Input:
			P_rlon: radians of longitude
			P_rlat: radians of latitude
			V_rlon: a vector of radians of longitude
			V_rlat: a vector of radians of latitude
		Output:
			distance: a vector of distance
		"""

		lon1 = P_rlon
		lat1 = P_rlat
		lon2 = V_rlon
		lat2 = V_rlat

		dlon = lon2 - lon1
		dlat = lat2 - lat1

		a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)**2
		c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
		distance = 6373.0 * c * 1000

		return distance



				
```

```{python, eval = FALSE}
def read_setting_from_txt(file_path = "setting.txt"):
	"""
	Read setting from the txt file.
	Input: 
		file_path: file_path and name of the csv file
	Output:
		active_time: length of time bushfires remain active
		adj_distance: density-based distance used in the clustering algorithm
	"""

	with open("scripts/Clustering/setting.txt", "r") as f:
		dic = {"active_time": None, "adj_distance": None}

		for line in f:
			line_string = line.split('=')
			if "active_time" in line_string[0].lower():
				dic['active_time'] = int(line_string[1].replace(' ','').replace('\n',''))

			if "adj_distance" in line_string[0].lower():
				dic['adj_distance'] = int(line_string[1].replace(' ','').replace('\n',''))

		if None in dic.values():
			raise Exception("Either value of active_time or adj_distance doesn't exist.")

		return dic['active_time'], dic['adj_distance']
		

```

```{python, eval = FALSE}
from os import remove

def main():

	ID, lon, lat, time_id = read_hotspots_from_csv()

	try:
		remove("data/clustering_grid.csv")
	except OSError:
		pass

	with open('data/clustering_grid.csv', 'a') as f:
		f.write("active_time,adj_dist,count\n")

	for active_time in [x * 3 for x in range(1, 13)]:
		for adj_dist in [x * 1000 for x in range(1, 11)]:

			print("Running", active_time, adj_dist)
	
			clusterer = CLUSTERER(
				active_time = active_time,
				adj_dist = adj_dist
				)

			clusterer.cluster(
				ID = ID, 
				lon = lon, 
				lat = lat, 
				time_id = time_id
				)

			count = max(clusterer.memberships.tolist())

			with open('data/clustering_grid.csv', 'a') as f:
				f.write("{},{},{}\n".format(active_time, adj_dist, count))

			print("count:", count)

			del clusterer





if __name__ == "__main__":
	main()
```

### choose optimal parameters for clustering (to get setting.txt)
```{r, eval = FALSE}
system("Rscript scripts/Clustering/clustering_tune_vis.R")
```

----from VIC_hotspot_before_clustering.csv to VIC_hotspot_after_clustering.csv-----

```{r Hotspots-Clustering, eval = FALSE}
# Cluster hotspots into bushfires
# Runtime: 6 minutes
# Input: VIC_hotspots_before_clustering.csv
# Output: VIC_hotspots_after_clustering.csv, summary.txt
  system("python scripts/Clustering/main.py")

```

### getting the predict_x_new.csv
```{r, eval = FALSE}
system("Rscript scripts/Training/combine_current.R")
```